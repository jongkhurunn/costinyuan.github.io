<!DOCTYPE html>
<html lang="zh">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
    Dai et al. - 2021 - UP-DETR: Unsupervised Pre-training for Object Detection with Transformers - 原子集
    
    </title>
    <link rel="shortcut icon" href="media/16853432384271/facicon-64x64.png" type="image/png" />

    
    
    <link href="atom.xml" rel="alternate" title="原子集" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/style.min.css">
    <link rel="stylesheet" href="asset/css/doc.css">
    <script src="asset/app.js"></script>
</head>
  <body>
    <section class="hero">
      <div class="hero-head">
          <nav class="navbar" role="navigation" aria-label="main navigation">
              <div class="container">
              <div class="navbar-brand">
                
                <a target="_self" class="navbar-item " href="index.html">Home</a>
                
                <a target="_self" class="navbar-item " href="archives.html">Archives</a>
                

                <a role="button" id="navbarSNSRssSwitchBtn" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navbarSNSRssButtons">
                  <span aria-hidden="true"></span>
                  <span aria-hidden="true"></span>
                  <span aria-hidden="true"></span>
                </a>
              </div>
            
              <div id="navbarSNSRssButtons" class="navbar-menu">
                <div class="navbar-start">
                  
                </div>
            
                <div class="navbar-end">
                  <div class="navbar-item">
                    <!--buttons start-->
                    <div class="buttons">
                      
                        
                        
                        
                        <a href="https://github.com/jongkhurunn" target="_blank" title="github">
                            <span class="icon is-large has-text-grey-darker">
                               <svg class="svg-inline--fa fa-github fa-w-16 fa-lg" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github fa-lg"></i> -->
                            </span>
                          </a>
                        
                        
                      
                      <a href="https://weibo.com/u/2378315157" target="_blank" title="weibo">
                          <span class="icon is-large has-text-grey-darker">
                            <svg class="svg-inline--fa fa-weibo fa-w-16 fa-lg" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="weibo" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7 0 395.3 0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"></path></svg><!-- <i class="fab fa-weibo fa-lg"></i> -->
                          </span>
                      </a>
                      
                      <a href="atom.xml" target="_blank" title="RSS">
                          <span class="icon is-large has-text-black-bis">
                              <svg class="svg-inline--fa fa-rss fa-w-14 fa-lg" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="rss" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" data-fa-i2svg=""><path fill="currentColor" d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg><!-- <i class="fas fa-rss fa-lg"></i> -->
                          </span>
                      </a>
                    </div>
                    <!--buttons end-->

                  </div>
                </div>
                </div>
              </div>
            </nav>
      </div>

 <div class="hero-body ct-body"></div>
      
    </section>
    <section class="ct-body">
      <div class="container">
          <div class="columns is-variable bd-klmn-columns is-4 is-centered">
              <div class="column is-four-fifths">
                  <div class="post-body single-content">
                              
                    <div class="card-image">
                        <figure class="image">
                          <img src="media/16613241634568/image-20211019210625597.png">
                        </figure>
                    </div>
                    
                    <h1 class="title">
                            Dai et al. - 2021 - UP-DETR: Unsupervised Pre-training for Object Detection with Transformers   
                      </h1>
                     
                    
                      <div class="media">
                            
                            <figure class="media-left">
                              <p class="image is-48x48">
                                
                                  <img class="is-rounded" src="media/16853432384271/favicon-240x240.png">
                                
                              </p>
                            </figure>
                            
                            <div class="media-content">
                              <div class="content">
                                <p>
                                 <span class="date">2022/08/24</span>
                                  <span class="tran-posted-in">posted in</span>&nbsp; 
                                  
                                      <span class="posted-in"><a href='%E7%AC%94%E8%AE%B0.html'>笔记</a></span>
                                         
                                  

                                   
                                      
                                  <br />
                                  <span class="tran-tags">Tags:</span>&nbsp;
                                  
                                    <a class="tag is-link is-light" href='tag_Transformer.html'>#Transformer</a>
                                  
                                    <a class="tag is-link is-light" href='tag_Unsupervised.html'>#Unsupervised</a>
                                     

                                </p>
                              </div>
                            </div>
                         
                    </div>
                </div>
                  <article class="markdown-body single-content">
                    <h2><a id="about" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>About</h2>
<h3><a id="%E6%A0%87%E9%A2%98" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>标题</h3>
<p>UP-DETR: Unsupervised Pre-training for Object Detection with Transformers</p>
<p>UP-DETR: 针对目标检测的无监督预训练Transformer</p>
<h3><a id="%E5%8F%91%E8%A1%A8" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>发表</h3>
<p>CVPR 2021 Oral</p>
<h3><a id="%E4%BD%9C%E8%80%85" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>作者</h3>
<p>Zhigang Dai ：华南理工大学</p>
<p>Bolun Cai ：腾讯微信AI</p>
<p>Yugeng Lin ：腾讯微信AI</p>
<p>Junying Chen* ：华南理工大学</p>
<h3><a id="%E4%BB%A3%E7%A0%81%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>代码/预训练模型</h3>
<p><a href="https://github.com/dddzg/up-detr">https://github.com/dddzg/up-detr</a></p>
<h2><a id="content" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Content</h2>
<h3><a id="%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>背景介绍</h3>
<h4><a id="detr" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>DETR</h4>
<p><img src="media/16613241634568/16852926816187.png" alt="" /></p>
<p>是一种用来目标检测的transformer编码-解码结构，含有：</p>
<p>1）预训练的CNN（ResNet50 with 23.2M parameters）</p>
<p>2）没有预训练的<a href="http://peterbloem.nl/blog/transformers">Transdormer from scratch</a>（<a href="https://zhuanlan.zhihu.com/p/351558402">Vanilla transformer</a> with 18.0M parameters）</p>
<p>通过对不同的对象查询（object queries）学习专属空间，得到不同的对象查询的坐标和盒子大小</p>
<h4><a id="%E6%97%A0%E7%9B%91%E7%9D%A3%EF%BC%88%E8%87%AA%E7%9B%91%E7%9D%A3%EF%BC%89%E9%A2%84%E8%AE%AD%E7%BB%83" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>无监督（自监督）预训练</h4>
<p>比如 BERT（NLP）的masked language model、MoCo（CV）的instance discrimination，通过一定的方式，从样本中无监督的构造一个 label</p>
<p>关键点：设计合理的pretext task</p>
<h3><a id="%E5%8A%A8%E6%9C%BA" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>动机</h3>
<p>既然CNN可以是无监督训练的，那么transformer能不能也做个无监督预训练呢</p>
<h3><a id="%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>存在的问题</h3>
<p>现存的用于CNN的pretext task不能直接应用于预训练DETR的transformers</p>
<blockquote>
<p>主要原因是DETR中的transformer主要是用来做空间信息上的定位而MoCo的pretext主要是用来提高CNN的物体鉴别能力</p>
</blockquote>
<h3><a id="%E6%83%B3%E6%B3%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>想法</h3>
<blockquote>
<p>受无监督学习在NLP领域中的极大成功的启发，我们的无监督预训练也用<strong>超大的训练集</strong>（ImageNet），把目标检测作为我们的下游任务（downstream task），提出了新的pretext task —— random query patch detection</p>
</blockquote>
<p>随机框若干个patch下来，把这些patch输入到decoder，原图输入到encoder，整个任务就变成了给定patch找他们在图中的位置。 </p>
<p>对于一个无监督训练好的DETR，只要输入patch，他就能做到无监督定位patch的功能（不需要额外的<a href="https://blog.csdn.net/just_sort/article/details/103308883">NMS</a>后处理），这个patch还能支持数据增强和尺度变换。</p>
<p><img src="media/16613241634568/16852927809090.png" alt="" /></p>
<p>两个难点：</p>
<ol>
<li>
<p>多任务学习 Multi-task learning</p>
<ul>
<li>
<p>对象检测是对象分类和定位的耦合（两个task）</p>
</li>
<li>
<p>为了避免query patch检测破坏分类特征，我们引入了”冻结预训练主干网络“（frozen pre-training backbone）和”patch特征重建“（patch feature reconstruction）</p>
</li>
<li>
<p>冻结预训练主干网络：固定预训练的CNN的权重</p>
</li>
<li>
<p>patch特征重建：使经过transformer的特征能保持和经过CNN后的特征保持一致的分类判别性</p>
</li>
</ul>
</li>
<li>
<p>多查询定位 Multi-query localization</p>
<ul>
<li>
<p>一张图内可能有多个对象，当对象查询object query太大时会很难收敛</p>
</li>
<li>
<p>随机设置M个query patch，并分配至100个embedding</p>
</li>
<li>
<p>提出了一个放在解码器上的attention mask，以确保query之间框的预测独立</p>
</li>
<li>
<p>提出了object query shuffle方法，以确保embedding和query patch的随机性</p>
</li>
</ul>
</li>
</ol>
<h3><a id="up-detr" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>UP-DETR</h3>
<h4><a id="1%EF%BC%89pre-training" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1）pre-training</h4>
<blockquote>
<p>Signle-query Patch</p>
</blockquote>
<p><img src="media/16613241634568/16852927809144.png" alt="" /></p>
<ol>
<li>
<p>将输入图像输入CNN主干神经网络中获得图像特征，加入位置编码输入tranformer</p>
</li>
<li>
<p>随机抓一块query patch，通过全局池化的CNN网络得到patch的特征，展平后分别加上object query形成N对预测框</p>
</li>
<li>
<p>计算N对预测框的相同匹配代价（same match cost），使用匈牙利算法计算真值（ground-truth）</p>
</li>
<li>
<p>损失函数（Hungarian loss）：交叉墒误差（match or not）+ （if match）\(l_1\) with IoU + （if match）重建损失</p>
<p><img src="media/16613241634568/image-20211020001114648.png" alt="image-20211020001114648" /></p>
</li>
</ol>
<blockquote>
<p>Multi-query Patches</p>
</blockquote>
<p><img src="media/16613241634568/image-20211019210625597.png" alt="image-20211019210625597" /></p>
<ol>
<li>把N个对象查询分成M组，每个query patch被分配给N/M个对象查询</li>
<li>对象查询嵌入会随机洗牌query shuffle</li>
<li>“注意力遮罩”加在自注意力解码器的softmax层中，如果两个对象查询相互影响就会遮挡</li>
<li>其他的都和Signle-query Patch一样</li>
</ol>
<h4><a id="2%EF%BC%89fine-tuning-procedures" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2）fine-tuning procedures</h4>
<h3><a id="%E5%AE%9E%E9%AA%8C" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>实验</h3>
<h4><a id="pre-training-setup" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>pre-training setup</h4>
<p>1）训练了ImageNet上1.28M个无标签训练集，使用Res-Net50作为CNN骨干网络使用SwAV方法进行无监督训练。在训练UP-DETR中，冻结CNN参数</p>
<p>2）放大ImageNet里的图片尺寸到[320:480, :600]，随机patch的xywh，然后调整到128x128</p>
<p>3）使用SimCLR-style的transformer结构，使用AdamW来优化UP-DETR，初始学习率为0.0001，权重下降为0.0001</p>
<p>4）使用大小为256的mini-batch在8个V100上训练了60epochs，在40epochs的时候学习率*0.1</p>
<h4><a id="fine-tuning-setup" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>fine-tuning setup</h4>
<p>使用VOC和COCO进行参数微调，tranformers的学习率设为0.0001，CNN骨干网络的学习率设为0.00005，其他都和DETR一样：8个v100 每个处理4张图</p>
<p>short：150 epochs，在100epochs的时候 lr*0.1</p>
<p>long：300 epochs，在200的时候 lr*0.1</p>
<h4><a id="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%AF%B9%E6%AF%94" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>目标检测结果对比</h4>
<blockquote>
<p><a href="https://blog.csdn.net/xys430381_1/article/details/90770520">AP</a>：如果检测框与groud-truth框的IOU区域大于某个阈值，就可以认为是true positive</p>
<p>如果IoU阈值=0.5，则为 \(AP_{50}\)</p>
<p>如果是对小物体（area&lt;32），则为 \(AP_m\)，以此类推</p>
</blockquote>
<ul>
<li>Faster R-CNN / DETR / UP-DETR 在相同数据集上目标检测结果</li>
</ul>
<p><img src="media/16613241634568/image-20211020011353161.png" alt="image-20211020011353161" /></p>
<ul>
<li>多种模型/骨干网络/epochs/数据集交叉比较</li>
</ul>
<p><img src="media/16613241634568/image-20211020012203042.png" alt="image-20211020012203042" /></p>
<ul>
<li>不同训练集150epochs和300epochs下DETR和UP-DETR学习曲线对比</li>
</ul>
<p><img src="media/16613241634568/image-20211020085828579.png" alt="image-20211020085828579" /></p>
<ul>
<li>single-query和multi-query对比</li>
</ul>
<img src="media/16613241634568/16852927808948.png" alt="image-20211020115154617" style="zoom:50%;" />
<ul>
<li>DETR、不使用冻结主干网络的UP-DETR（ac）、冻结主干网络的UP-DETR(bd)</li>
</ul>
<img src="media/16613241634568/16852927809040.png" alt="image-20211020120034426" style="zoom:50%;" />
<h4><a id="%E5%8F%AF%E8%A7%86%E5%8C%96" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>可视化</h4>
<p>1）手动裁剪图像中的物体patch，并对其进行SimCLRstyle数据增强</p>
<p>2）将这些patch作为query object送入模型，最后将模型的输出与边框进行可视化处理</p>
<p>这个过程可以看做是无监督的一次性测试或基于生度学习的模版匹配</p>
<p><img src="media/16613241634568/16852927809397.png" alt="image-20211020170121560" /></p>
<h3><a id="%E7%BB%93%E8%AE%BA" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>结论</h3>
<ul>
<li>
<p>最近关于无监督预训练的研究主要集中在对比学习的特征识别上，而不是空间定位的专门模块。</p>
</li>
<li>
<p>在UP-DETR预训练中，预训练任务主要是通过位置编码和可学习的对象查询来设计补丁定位。</p>
</li>
<li>
<p>future work：一种先进的方法能够将CNN和变换器的预训练整合到一个统一的端到端框架中，并将UP-DETR应用到更多的下游任务中</p>
</li>
</ul>

                  </article>
                  <div class="comments-wrap">
                    <div class="share-comments">
                      

                      

                      
                    </div>
                  </div><!-- end comments wrap -->
              </div>
            </div><!-- end columns -->
      </div><!-- end container -->
    </section>



    <footer class="footer">
        <div class="content has-text-centered">
          <p>
              Copyright &copy; 2019
              Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
              Theme used <a target="_blank" href="https://bulma.io/">Bulma CSS</a>.
          </p>
        </div>
      </footer>



<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

  















  
    




  </body>
</html>
